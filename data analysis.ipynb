{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d747b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5653a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Muzhen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e6a712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agree #</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>\"Kids grow really fast and a lot of people bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>\"A lot of the op shops in Perth tend to check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>\"Really depends on the area and the staff's g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>\"They were half of half and also on clearance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>\"How our used clothing stores mark everything...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agree #                                        review_text\n",
       "0      132   \"Kids grow really fast and a lot of people bu...\n",
       "1        3   \"A lot of the op shops in Perth tend to check...\n",
       "2       21   \"Really depends on the area and the staff's g...\n",
       "3        2   \"They were half of half and also on clearance...\n",
       "4       31   \"How our used clothing stores mark everything..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1= pd.read_csv('reddit review.csv', encoding=\"cp1252\")\n",
    "#df1.head()\n",
    "df1= pd.read_csv('/Users/Muzhen/Downloads/reddit review.csv', encoding=\"cp1252\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b051f7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892\n"
     ]
    }
   ],
   "source": [
    "df_1=df1\n",
    "print(df1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162248b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'its': 3, 'a': 2, 'man': 1, 'plane': 1, 'superman': 1}\n"
     ]
    }
   ],
   "source": [
    "def word_count(sentence):\n",
    "    translate = sentence.maketrans({char: None for char in \"'.,:*!\"})\n",
    "    cleaned_words = sentence.lower().translate(translate).split()\n",
    "    word_counter = {}\n",
    "    for word in cleaned_words:\n",
    "        if word in word_counter:\n",
    "            word_counter[word] += 1\n",
    "        else:\n",
    "            word_counter[word] = 1\n",
    "    return word_counter\n",
    "example = word_count(\"It's a man, it's a plane, it's superman!\")\n",
    "print (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56458af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Kids grow really fast and a lot of people buy a few season's worth\", \n",
      "{'\"kids': 1, 'grow': 1, 'really': 1, 'fast': 1, 'and': 1, 'a': 2, 'lot': 1, 'of': 1, 'people': 1, 'buy': 1, 'few': 1, 'seasons': 1, 'worth\"': 1}\n",
      "\n",
      " \"A lot of the op shops in Perth tend to check the original price of any brand name items online now and price accordingly, mostly to cash in on the profits that others are making by buying their goods cheap and flogging them on \\\"Instagram boutiques\\\"\", \n",
      "{'\"a': 1, 'lot': 1, 'of': 2, 'the': 3, 'op': 1, 'shops': 1, 'in': 2, 'perth': 1, 'tend': 1, 'to': 2, 'check': 1, 'original': 1, 'price': 2, 'any': 1, 'brand': 1, 'name': 1, 'items': 1, 'online': 1, 'now': 1, 'and': 2, 'accordingly': 1, 'mostly': 1, 'cash': 1, 'on': 2, 'profits': 1, 'that': 1, 'others': 1, 'are': 1, 'making': 1, 'by': 1, 'buying': 1, 'their': 1, 'goods': 1, 'cheap': 1, 'flogging': 1, 'them': 1, '\\\\\"instagram': 1, 'boutiques\\\\\"\"': 1}\n",
      "\n",
      " \"Really depends on the area and the staff's general level of knowledge. A lot of second hand stores in Australia are run by charities/volunteers, so mileage varies when it comes to pricing.\", \n",
      "{'\"really': 1, 'depends': 1, 'on': 1, 'the': 2, 'area': 1, 'and': 1, 'staffs': 1, 'general': 1, 'level': 1, 'of': 2, 'knowledge': 1, 'a': 1, 'lot': 1, 'second': 1, 'hand': 1, 'stores': 1, 'in': 1, 'australia': 1, 'are': 1, 'run': 1, 'by': 1, 'charities/volunteers': 1, 'so': 1, 'mileage': 1, 'varies': 1, 'when': 1, 'it': 1, 'comes': 1, 'to': 1, 'pricing\"': 1}\n",
      "\n",
      " \"How our used clothing stores mark everything brand name up! Those would be $29.99 at VV\", \n",
      "{'\"how': 1, 'our': 1, 'used': 1, 'clothing': 1, 'stores': 1, 'mark': 1, 'everything': 1, 'brand': 1, 'name': 1, 'up': 1, 'those': 1, 'would': 1, 'be': 1, '$2999': 1, 'at': 1, 'vv\"': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = word_count(df_1.loc[0,\"review_text\"])\n",
    "print (df_1.loc[0,\"review_text\"])\n",
    "print (example)\n",
    "print (\"\")\n",
    "\n",
    "example = word_count(df_1.loc[1,\"review_text\"])\n",
    "print (df_1.loc[1,\"review_text\"])\n",
    "print (example)\n",
    "print (\"\")\n",
    "\n",
    "example = word_count(df_1.loc[2,\"review_text\"])\n",
    "print (df_1.loc[2,\"review_text\"])\n",
    "print (example)\n",
    "print (\"\")\n",
    "\n",
    "\n",
    "example = word_count(df_1.loc[4,\"review_text\"])\n",
    "print (df_1.loc[4,\"review_text\"])\n",
    "print (example)\n",
    "print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97dbb999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \"Kids grow really fast and a lot of people buy a few seasons worth\", ',\n",
      " ' \"A lot of the op shops in Perth tend to check the original price of any '\n",
      " 'brand name items online now and price accordingly, mostly to cash in on the '\n",
      " 'profits that others are making by buying their goods cheap and flogging them '\n",
      " 'on \\\\\"Instagram boutiques\\\\\"\", ',\n",
      " ' \"Really depends on the area and the staffs general level of knowledge. A '\n",
      " 'lot of second hand stores in Australia are run by charities/volunteers, so '\n",
      " 'mileage varies when it comes to pricing.\", ',\n",
      " ' \"They were half of half and also on clearance. It was my lucky day lol\", ']\n"
     ]
    }
   ],
   "source": [
    "data = df_1.review_text.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d025310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kids', 'grow', 'really', 'fast', 'and', 'lot', 'of', 'people', 'buy', 'few', 'seasons', 'worth'], ['lot', 'of', 'the', 'op', 'shops', 'in', 'perth', 'tend', 'to', 'check', 'the', 'original', 'price', 'of', 'any', 'brand', 'name', 'items', 'online', 'now', 'and', 'price', 'accordingly', 'mostly', 'to', 'cash', 'in', 'on', 'the', 'profits', 'that', 'others', 'are', 'making', 'by', 'buying', 'their', 'goods', 'cheap', 'and', 'flogging', 'them', 'on', 'instagram', 'boutiques']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48a41c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'depends', 'on', 'the', 'area', 'and', 'the', 'staffs', 'general', 'level', 'of', 'knowledge', 'lot', 'of', 'second_hand', 'stores', 'in', 'australia', 'are', 'run', 'by', 'charities', 'volunteers', 'so', 'mileage', 'varies', 'when', 'it', 'comes', 'to', 'pricing']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(bigram_mod[data_words[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2d32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c47fee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d420a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kid', 'grow', 'really', 'fast', 'lot', 'people', 'buy', 'season', 'worth']]\n",
      "[['lot', 'op', 'shop', 'check', 'original', 'price', 'brand', 'name', 'item', 'online', 'price', 'accordingly', 'mostly', 'cash', 'profit', 'other', 'make', 'buy', 'good', 'cheap', 'flog', 'instagram', 'boutique']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "print(data_lemmatized[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22ba28eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (4, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1)], [(4, 1), (6, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)], [(41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abfd1798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3452076126185906\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.02,\n",
    "                                           eta=0.05,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18262a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=7, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                            eta=0.05,\n",
    "                                           per_word_topics=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e282d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b42e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.015*\"plastic\" + 0.014*\"brand\" + 0.012*\"years\" + 0.011*\"recycled\" + 0.011*\"eco-friendly\" + '\n",
      "  '0.011*\"marketing\" + 0.010*\"resale\" + 0.009*\"use\"'),\n",
      " (1,\n",
      "  '0.022*\"second\" + 0.018*\"store\" + 0.014*\"good\" + 0.010*\"hand\" + 0.009*\"thrift\" + '\n",
      "  '0.008*\"circular\" + 0.008*\"reuse\" + 0.008*\"search\"'),\n",
      " (2,\n",
      "  '0.018*\"fast\" + 0.012*\"buy\" + 0.011*\"fashion\" + 0.010*\"get\" + '0.009*\"claims\" + '\n",
      "  '0.009*\"brand\" + 0.008*\"greenwashing\" + 0.007*\"company\"'),\n",
      " (3,\n",
      "  '0.019*\"buy\" + 0.014*\"used\" + 0.013*\"new\" + 0.011*\"quality\" + 0.011*\"decision\" + '\n",
      "  '0.010*\"purchase\" + 0.010*\"value\" + 0.010*\"durability\"'),\n",
      " (4,\n",
      "  '0.015*\"leather\" + 0.013*\"vegan\" + 0.012*\"alternative\" + 0.011*\"reduce\" + '\n",
      "  '0.010*\"claim\" + 0.009*\"luxury\" + 0.009*\"options\" + 0.008*\"product\"'),\n",
      " (5,\n",
      "  '0.013*\"carbon\" + 0.011*\"ethical\" + 0.010*\"need\" + 0.009*\"offset\" + 0.009*\"environment\" + '\n",
      "  '0.009*\"emiison\" + 0.009*\"promise\" +  0.0009*\"supply\"'),\n",
      " (6,\n",
      "  '0.013*\"cotton\" + 0.011*\"like\" + 0.010*\"thrift\" + 0.009*\"fabric\" + 0.009*\"natural\" + '\n",
      "  '0.009*\"choice\" + 0.009*\"wardrobe\" + 0.0009*\"wear\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(num_words=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb01a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84582c-9fe0-4d8a-b2f9-7389e833b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accdf2c-8c94-434f-8e78-05b36a63ab85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
